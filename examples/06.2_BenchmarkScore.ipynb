{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 6\n",
    "## 6.1 Benchmark: Score\n",
    "Now that we have a convenient way to make recommendations and a convenient way to split our data into training and test sets, it is straightforward to benchmark our algorithms and find both the best one and the best settings for it.\n",
    "\n",
    "For that purpose, `bestPy` provides the `Benchmark` class, which brings together a fully configured recommender (_i.e._, a `RecoBasedOn` instance) and the test data to provide a score for the algorithm used in the recommender. Let's see how that works.\n",
    "\n",
    "\n",
    "### Preliminaries\n",
    "We only need this because the examples folder is a subdirectory of the `bestPy` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, logging, and data\n",
    "In addition to what we did in the last notebook, we now import also the `Benchmark` class from the top-level package. As an algorithm, we are going to chose `CollaborativeFiltering` and import also a two similarities for comparison. Importantly, we also need the `Baseline` to establish a basic score that any algorithm worthy of the name should beat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bestPy import write_log_to\n",
    "from bestPy.datastructures import TrainTest\n",
    "from bestPy import Benchmark, RecoBasedOn  # Additionally import Benchmark\n",
    "from bestPy.algorithms import Baseline, CollaborativeFiltering  # Import also Baseline for score to beat\n",
    "from bestPy.algorithms.similarities import kulsinski, cosine  # Import two similarities for comparison\n",
    "\n",
    "logfile = 'logfile.txt'\n",
    "write_log_to(logfile, 20)\n",
    "\n",
    "file = 'examples_data.csv'\n",
    "data = TrainTest.from_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the `TrainTest` data and set up a recommender with the ` Baseline` algorithm\n",
    "Let's stick with holding out the last 4 unique purchases from each customer and let's say we want to recommend also articles that customers bought before. After splitting the data accordingly, we are going to set up a recommender with the training data and the `Baseline` as algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.split(4, False)\n",
    "\n",
    "algorithm = Baseline()\n",
    "\n",
    "recommender = RecoBasedOn(data.train).using(algorithm).keeping_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new `Benchmark` object\n",
    "To instantiate the `Benchmark` class, a recommender object of type `RecoBasedOn` is required (think: \"_What_ do I want to benchmark?\"), like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark = Benchmark(recommender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tab inspection tells us that the newly created object has a single method `against()`. In order to provide a benchmark score for our recommender, which was trained on the training data, we also need the held out test data to test it _against_. So the argument of `against()` is the held-out test data and its return value is the benchmark object, now with test data attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark = benchmark.against(data.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beauty of this peculiar way of calling the `against()` method is again revealed when combining it with instantiation of the `Benchmark` class into a single, elegant line of code that reads like an instruction in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark = Benchmark(recommender).against(data.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring a recommender with the `Benchmark` object\n",
    "Now that we have set up a `Benchmark` object with a fully configured recommender, trained with training data, and pitched against the according test data, a new attribute `score` magically appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11555864949530108"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tells us that, on average, customers actually bought about 0.12 articles out of the 4 we recommended. And that's just the baseline recommendation that does not at all take into account differences between customers. Any algorithm that does should easily beat that number. Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4552732335537765"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm = CollaborativeFiltering()\n",
    "algorithm.binarize = False\n",
    "algorithm.similarity = cosine\n",
    "\n",
    "recommender = RecoBasedOn(data.train).using(algorithm).pruning_old\n",
    "\n",
    "benchmark = Benchmark(recommender).against(data.test)\n",
    "benchmark.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, using collaborative filtering, we could significantly improve over the customer-agnostic baseline and get about 0.46 articles out of 4 right. But maybe we should not count each individual purchase and, instead, just consider whether a customer bought an article or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4684998259658893"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm.binarize= True\n",
    "benchmark.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That improved our score a little. But we also have other knobs to turn. How about trying a different way of measuring similarity between articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49182039679777234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm.similarity = kulsinski\n",
    "benchmark.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better again! You can clearly see where this is going. How high can you get the score? Happy exploring!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: You may have realized that, when we split the data, we told `bestPy` that we would recommend back to customers also articles that they bought before.\n",
    "```\n",
    "data.split(4, False)\n",
    "```\n",
    "\n",
    "\n",
    "When setting up our recommender with collaborative filtering, however, we wrote\n",
    "```\n",
    "recommender = RecoBasedOn(data.train).using(algorithm).pruning_old\n",
    "```\n",
    "meaning that we would _not_, in fact recommend previously bought articles. That's a clear contradiction. Which one is it going to be? Because, in this context, the way we have split the data determines the way we have to test it, the value of the `only_new` attrribute of the test data takes precedence over the value of the same attribute of the recommender object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender before: True\n",
      "Test data: False\n",
      "Recommender after: False\n"
     ]
    }
   ],
   "source": [
    "recommender = RecoBasedOn(data.train).using(algorithm).pruning_old\n",
    "print('Recommender before:', recommender.only_new)\n",
    "print('Test data:', data.test.only_new)\n",
    "\n",
    "benchmark = Benchmark(recommender).against(data.test)\n",
    "print('Recommender after:', recommender.only_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reset conveniently takes place behind the scenes but, to keep you informed of what's happening at all times, it is of course logged.\n",
    "```\n",
    "[INFO    ]: Resetting recommender to \"keeping_old\" because of test-data preference. (benchmark|against)\n",
    "```\n",
    "\n",
    "And this concludes our discussion of how to benchmark and tune `bestPy`'s algorithms.\n",
    "\n",
    "Once you found the optimal algorithm and settings, why not provide your recommendations as a service _via_ a RESTful API? Check out the next chapter `07_RESTfulAPI.py`, which is _not_ a `jupyter` notebook but a regular python script. So just open it in a text editor (or an IDE) to read the ample documentation or execute it on the command line by typing\n",
    "```\n",
    "python 07_RESTfulAPI.py\n",
    "```\n",
    "on your shell prompt to see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
